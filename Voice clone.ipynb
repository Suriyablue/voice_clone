{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjZKnR5zDOde"
   },
   "source": [
    "## üîß Installing Required Text-to-Speech Library\n",
    "We install the `TTS` library (by coqui.ai), which is used for text-to-speech conversion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6QNvuQLzLfhC",
    "outputId": "10d393b8-fbe7-4459-b786-adf105965259"
   },
   "outputs": [],
   "source": [
    "# Installing the Coqui TTS library for speech synthesis\n",
    "!pip install TTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnm3k0MoDhRv"
   },
   "source": [
    "## üîß Installing Resemblyzer for Voice Embeddings\n",
    "We install `resemblyzer`, a library used for extracting speaker embeddings from audio. This is useful for speaker verification or voice cloning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nf9w_18LPrek",
    "outputId": "4b64ab25-0fd7-4847-b7e2-eb725dbbc713"
   },
   "outputs": [],
   "source": [
    "# Installing Resemblyzer library to extract speaker embeddings\n",
    "!pip install resemblyzer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6o_vSwBlD5ed"
   },
   "source": [
    "## üîß Installing Additional Audio Processing Libraries\n",
    "We install key libraries for audio preprocessing and transcription, including `faster-whisper`, `pydub`, `librosa`, and `soundfile`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4YM75SwxP46X",
    "outputId": "33161906-1362-4ffc-bcd2-0b73834a690c"
   },
   "outputs": [],
   "source": [
    "# Installing whisper-based ASR (Automatic Speech Recognition) and audio processing tools\n",
    "!pip install faster-whisper pydub librosa soundfile --quiet  # --quiet reduces console output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8Bl2nn-EURz"
   },
   "source": [
    "## üéµ Audio Preprocessing: Convert to Mono and Resample\n",
    "We load the input audio file, convert it to mono, resample it to 22050 Hz, and save the preprocessed output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "atpxU6xgQh1Y",
    "outputId": "58a3e4b8-c318-4f2c-aecb-df9a192d4597"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "# Define file paths\n",
    "input_path = \"/content/drive/MyDrive/AudioFiles/uml diagrams using chatgot_082553-enhanced-v2.wav\"\n",
    "output_path = \"/content/drive/MyDrive/AudioFiles/output.wav\"\n",
    "\n",
    "# Load the audio: convert to mono and resample to 22050 Hz\n",
    "wav, sr = librosa.load(input_path, sr=22050, mono=True)\n",
    "\n",
    "# Save the preprocessed audio to a new file\n",
    "sf.write(output_path, wav, 22050)\n",
    "\n",
    "print(f\"‚úî Preprocessed audio saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d62RP6nqEpAR"
   },
   "source": [
    "## üß† Extracting Speaker Embedding using Resemblyzer\n",
    "We use `Resemblyzer` to extract a speaker embedding (also called a d-vector) from the preprocessed audio. This embedding represents the unique vocal features of the speaker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zhzfnQekRUoG",
    "outputId": "240c01b5-5962-4487-bea1-3f53656820c5"
   },
   "outputs": [],
   "source": [
    "from resemblyzer import VoiceEncoder, preprocess_wav\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the voice encoder\n",
    "encoder = VoiceEncoder()\n",
    "\n",
    "# Preprocess the audio file for embedding extraction\n",
    "wav_pre = preprocess_wav(output_path)\n",
    "\n",
    "# Extract speaker embedding (d-vector)\n",
    "embed = encoder.embed_utterance(wav_pre)\n",
    "\n",
    "print(\"‚úî Speaker embedding extracted. Shape:\", embed.shape)\n",
    "\n",
    "# Save the embedding as a .npy file for future use\n",
    "np.save(\"/content/drive/MyDrive/AudioFiles/embeddings.npy\", embed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1DTJ8vwE256"
   },
   "source": [
    "## üó£Ô∏è Voice Cloning with Pre-trained Multilingual TTS Model\n",
    "We use the Coqui TTS library to generate speech from text, mimicking the voice characteristics of a reference speaker using the extracted embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aUoypE1YRd8u",
    "outputId": "53b55ef5-a6e0-4f89-afe2-fc5321bb9c3f"
   },
   "outputs": [],
   "source": [
    "from TTS.api import TTS\n",
    "\n",
    "# Load a pre-trained multilingual TTS model capable of voice cloning\n",
    "tts = TTS(model_name=\"tts_models/multilingual/multi-dataset/your_tts\", progress_bar=False, gpu=False)\n",
    "\n",
    "# Define the text to synthesize\n",
    "text = \"Hello Every one welcome to MG squad naa thaa unga madan\"\n",
    "output_audio = \"cloned.wav\"\n",
    "\n",
    "# Generate cloned speech using the reference audio and speaker embedding\n",
    "tts.tts_to_file(\n",
    "    text=text,\n",
    "    speaker_wav=output_path,           # Reference audio (used for voice cloning)\n",
    "    file_path=output_audio,            # Output path for cloned audio\n",
    "    language='en',                     # Language code\n",
    "    speaker_embedding=embed            # Speaker embedding vector\n",
    ")\n",
    "\n",
    "print(f\"‚úî Cloned voice saved to: {output_audio}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEHGNas3GCOZ"
   },
   "source": [
    "## ‚ñ∂Ô∏è Play and Download the Cloned Audio\n",
    "We play the generated cloned audio within the notebook and also provide a download link to save it locally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "4IFI6sfmRw25",
    "outputId": "184e2a2c-0fdc-41fe-e194-a7236cf08d41"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio, FileLink\n",
    "\n",
    "# Play the cloned audio\n",
    "Audio(\"cloned.wav\")\n",
    "\n",
    "# Provide a download link to manually download the file\n",
    "FileLink(\"cloned.wav\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOxGXRLbGj2t"
   },
   "source": [
    "## üßæ Transcribing Audio Using Faster-Whisper\n",
    "We use the `faster-whisper` ASR model to transcribe the input audio into text. The transcription is done segment-wise and then combined into a single string.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272,
     "referenced_widgets": [
      "a4053bad11a84c23b16e20bc9ddd66e2",
      "9a86d5786ac44fcb98018bc1eee9e7fa",
      "64737e1d00504cc58be5e53972e12627",
      "a88f1c71fe414fe19916adf39d4a3f34",
      "499082ba961744dbbcf50dc6eb24b73c",
      "2660a5d13b884bc29b994716f12c0746",
      "24ba86366012445c918fa3b4f3827787",
      "ae95f1e65b6b45a78798e08eff52bcb9",
      "ebaae5e812af41099e43da6c4164a35a",
      "691475d3906f42078d3cbd1a3213d25d",
      "6a77521358a4444cb4336750f137c12e",
      "27fb3e360f894580b5bbec15d75111d8",
      "627d391f5651430ba3300470605eda9d",
      "71a074c807a848149a754425eac24a65",
      "6ce2713f76d44cfd95a6621b42379f12",
      "7b15e93583ff4cbea3de3a17efbf1089",
      "1482b8a2a91b4cdcbba5201c3d67fbcd",
      "f914f4d03ca540ea969649d4cdc99f26",
      "1029fc54ee9f4ca1bda490f7a5442773",
      "eca1f23881ee483086ead6eb113fee62",
      "df50dda8d5e5457480a0fb9be8ff643a",
      "4cfa48ac883d446da7f41a55397b105e",
      "6876ae6c88124f1493a2aea7cf00fec9",
      "cce0c0a508c149fea5f20297508424c9",
      "298128c56d114b7e959e8fd62cb4111e",
      "cf3452aab62f49f1953e9ff6fede80a1",
      "20aa7b860dc445f3b4d70c79a4a65595",
      "fa7a14478a3a4add805b094d944333aa",
      "bdb56a775adb410e9f04782e5f737a8f",
      "cb0319cf2c2443b3a79771c1696d8607",
      "3b1fbf4252934649b7e4635f2c722729",
      "ffb152c3376f4dd4813a4632fe40b9c5",
      "01436c006260496db994e83e28f7b1b5",
      "01a0af82c1c746bc9a3aa785dfc42eb8",
      "f08d6c205fc14eefa56ceae8481ca2ca",
      "113a87d0b9c247fdb1dd96f9cac4ac3c",
      "0c2935af731747eb9a1a656e27863ada",
      "a74ec41f366d4d3fa620f7e9c0d9eed5",
      "883119e03bb54034a2d311da41f8837f",
      "a4a8316ba1184f719c70a451bac4ee70",
      "e7f3b4ac0b2d4507a39cc202abfa69e9",
      "bd3383e914d0411cb163a182831d0274",
      "1935e7df57094f778cfdb7ea016b1a61",
      "352746caa08749caa4fb660db256ec9c"
     ]
    },
    "id": "3RdpJuljYYUE",
    "outputId": "bba6cc2e-85a3-4f9b-a8b0-4856d21da152"
   },
   "outputs": [],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "\n",
    "# Load Whisper model\n",
    "model = WhisperModel(\"small\", compute_type=\"auto\")  # use 'medium' or 'small' if needed\n",
    "\n",
    "# Transcribe with forced language as English\n",
    "segments, _ = model.transcribe(\n",
    "    \"/content/drive/MyDrive/AudioFiles/Vijay-Mallya-UGLY-TRUTH-Madan-Go.wav\",\n",
    "    language=\"en\"  #Force the model to interpret speech as English\n",
    ")\n",
    "\n",
    "# Combine all segments into one text\n",
    "transcribed_text = \" \".join([seg.text for seg in segments])\n",
    "print(\"üìù Transcribed Text:\\n\", transcribed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 688
    },
    "id": "S6eAv4_v7De4",
    "outputId": "4287746c-b65e-4a51-f421-9edf3d430926"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load original and cloned audio\n",
    "orig_audio_path = \"/content/drive/MyDrive/AudioFiles/output.wav\"\n",
    "cloned_audio_path = \"cloned.wav\"\n",
    "\n",
    "orig_audio, sr_orig = librosa.load(orig_audio_path, sr=22050)\n",
    "cloned_audio, sr_clone = librosa.load(cloned_audio_path, sr=22050)\n",
    "\n",
    "# Trim to same length (based on the shorter audio)\n",
    "min_len = min(len(orig_audio), len(cloned_audio))\n",
    "orig_audio = orig_audio[:min_len]\n",
    "cloned_audio = cloned_audio[:min_len]\n",
    "\n",
    "# Compute MFCCs\n",
    "mfcc_orig = librosa.feature.mfcc(y=orig_audio, sr=22050, n_mfcc=13)\n",
    "mfcc_clone = librosa.feature.mfcc(y=cloned_audio, sr=22050, n_mfcc=13)\n",
    "\n",
    "# Plot MFCCs side-by-side\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "librosa.display.specshow(mfcc_orig, sr=22050, x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title(\"üé§ Original Audio MFCC\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "librosa.display.specshow(mfcc_clone, sr=22050, x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title(\"üó£Ô∏è Cloned Audio MFCC\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cosine similarity between mean MFCCs\n",
    "sim_score = cosine_similarity(\n",
    "    [np.mean(mfcc_orig, axis=1)],\n",
    "    [np.mean(mfcc_clone, axis=1)]\n",
    ")[0][0]\n",
    "\n",
    "print(f\"‚úÖ Similarity Score (0 to 1): {sim_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7u8Q9PyFASIk",
    "outputId": "1c411eb8-6506-4dd0-97bb-395ce85354e1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
